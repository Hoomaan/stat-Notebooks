{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "#If you don't have nltk installed it via : pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for the first time, download either all the packages or the popular ones\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon and Corpus\n",
    "Naturall Language Processing (NLP) Has its own jargons and lexicon:\n",
    "Lexicon: the vocabulary of a person, language, or branch of knowledge. For example English lexicon, Medical English lexicon\n",
    "Corpus(plural corpora) is a large and structured set of texts. A list of several corpora can be found here: https://www.english-corpora.org/\n",
    "Words can have different common meaning in different lexicons\n",
    "example:\n",
    "Signal: In engineering: a signal represents a set of values\n",
    "Signal: In daily English: A sign conveying information\n",
    "### Tokenization\n",
    "Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, My name is J. Doe.', 'What can I do for you?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "s='Hello, My name is J. Doe. What can I do for you?'\n",
    "sL1=sent_tokenize(s)\n",
    "print(sL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'My', 'name', 'is', 'J.', 'Doe', '.', 'What', 'can', 'I', 'do', 'for', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "sL2=word_tokenize(s)\n",
    "print(sL2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the Corpus bring you?\n",
    "### Example: Stop Words\n",
    "In computing, stop words are words which are filtered out before processing of natural language data (text).[1] Stop words are generally the most common words in a language; there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "{\"mustn't\", 'or', 'wasn', \"hadn't\", 'up', 'it', 'this', 'll', 'other', \"mightn't\", 'themselves', 'before', 'whom', 'isn', 'your', 's', 'for', 'too', \"shouldn't\", 'how', 'where', 'i', \"it's\", 'then', 'if', 'until', 'have', 'are', 'myself', 'above', 'very', 'all', 'below', 'because', 'such', 'by', 'those', 'its', 'there', 'when', 'between', 'did', \"needn't\", 'few', 'do', 'only', \"weren't\", 'him', 'more', 'at', 'further', 'haven', 'here', \"should've\", 'didn', 'an', 'be', 'a', \"shan't\", 'them', \"won't\", 'during', 'should', 'is', 'been', 'each', 'as', \"you'll\", 'needn', 'than', 'herself', 'hadn', 'me', 'once', 'some', 'against', 've', 'has', \"you've\", 'down', 'her', 'yourself', 'the', 'about', 'yourselves', 'aren', 'after', 'he', 'most', 'was', 'just', 'd', 'while', 'had', 'in', 'what', 'doing', 'shan', 'of', 'with', \"you're\", 'o', 'his', 'these', 'under', \"hasn't\", 'from', 'hers', \"aren't\", 'y', 'and', 'that', 'over', \"doesn't\", 'weren', 'ain', 'they', \"wasn't\", 'into', 'on', 'through', 'off', 'theirs', \"you'd\", 'not', \"don't\", 'mightn', 'shouldn', 'wouldn', 'don', \"didn't\", 'couldn', 'no', 'so', \"wouldn't\", 'we', 'who', 'ourselves', 't', 'any', 'you', 'yours', 'ma', 'won', 'both', 'doesn', 'were', \"she's\", 'hasn', 'to', 're', 'can', 'our', 'again', 'nor', \"haven't\", \"that'll\", 'am', 'mustn', 'she', 'himself', 'will', 'm', 'having', 'but', 'being', 'does', 'why', 'out', 'own', 'my', \"couldn't\", \"isn't\", 'itself', 'which', 'now', 'their', 'ours', 'same'}\n",
      "\n",
      "\n",
      "All the words\n",
      "['Hello', ',', 'My', 'name', 'is', 'J.', 'Doe', '.', 'What', 'can', 'I', 'do', 'for', 'you', '?']\n",
      "\n",
      "\n",
      "After Removing stop words\n",
      "['Hello', ',', 'My', 'name', 'J.', 'Doe', '.', 'What', 'I', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set( stopwords.words(\"english\"))\n",
    "print('Stop words')\n",
    "print(stopWords)\n",
    "def removeFromList(x,y):\n",
    "    return [item for item in x if item not in y]\n",
    "print('\\n\\nAll the words')\n",
    "print(sL2)\n",
    "print('\\n\\nAfter Removing stop words')\n",
    "print(removeFromList( sL2,stopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding stem of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer \t\t SnowballStemmer\n",
      "bigger \t\t bigger\n",
      "higher \t\t higher\n",
      "highli \t\t high\n",
      "proper \t\t proper\n",
      "properli \t\t proper\n",
      "think \t\t think\n",
      "think \t\t think\n",
      "thinker \t\t thinker\n",
      "ride \t\t ride\n",
      "ride \t\t ride\n",
      "rider \t\t rider\n",
      "rid \t\t rid\n",
      "rid \t\t rid\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "words=[\"bigger\",\"higher\",\"highly\",\"proper\",\"properly\",\"think\",\"thinking\",\"thinker\",  \"ride\",\"riding\",\"rider\" ,\"rid\",\"ridding\"]\n",
    "print(\"PorterStemmer\",\"\\t\\t\",\"SnowballStemmer\")\n",
    "for w in words:\n",
    "    print( PorterStemmer().stem(w),'\\t\\t',SnowballStemmer(\"english\").stem(w) )\n",
    "#Don't have high expectation from the stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has *-2 to close at 2645.90 in moderate trading , a large addition or\n",
      "subtraction to a new public spirit for school betterment . *-1 to beat\n",
      "the tests themselves that critics say 0 Columbia 's junk problems\n",
      "*T*-1 . Sea Containers is still : ` * Buy , ' '' he said *T*-2 Mr.\n",
      "Guffey , `` and I 'm afraid 0 I reached *T*-1 . After the first 0 it\n",
      "believes 0 the Big Board 's leadership -- over the next 12 months --\n",
      "vs. a `` reckless course of action '' for President Bush must decide\n",
      "whether\n"
     ]
    }
   ],
   "source": [
    "s=text7.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
